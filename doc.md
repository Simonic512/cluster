决策树是一个预测模型，代表了对象的属性与目标值之间的映射关系。树中的每个节点都代表了一个相应的特征，每个节点分叉的路径代表了某个可能的取值或者连续值得取值范围，每个叶节点则直接代表分类结果（目标属性的值）。可见决策树的构成有几大要素：决策节点，方案枝，状态节点，概率枝（决策结果）。决策树具有可理解性好，计算量小，可以处理离散字段等优点，
当决策树结构建立起之后，使用决策树进行分类，只需要对相应的属性进行比对，在每个分叉节点上进行选择，最终就能选取到相应的叶节点作为数据的分类结果。
决策树的结构通常使用数据集进行训练得到，这个过程叫做决策树的学习。

决策树学习有很多方法，关键的问题都在于选择合适的特征作为判断节点，生成相应的方案枝，然后使用同样的方式迭代生成整个树结构。特征选择的目的就是在进行划分后能够更为明显的区分几个类别，或者是能够获得最大的信息增益，或者是在划分之后的几个集合都有比较高的相似度（纯度）因此有不同的划分公式，也产生了C45,ID3,CART等不同的决策树算法。比如信息增益划分法，信息熵表示的是集合的不确定度，均匀分布时，不确定度最大，因此熵就最大，而信息增益表示的是集合在按照此种划分方式前后信息熵的变化情况。信息增益划分法遍历所有的特征计算相应的信息增益，然后选择信息增益最大的特征作为此次划分的特征。
划分前数据集的信息熵计算公式：
其中 D 表示训练数据集，c 表示数据类别数，Pi 表示类别 i 样本数量占所有样本的比例。

对应数据集 D，选择特征 A 作为决策树判断节点时，在特征 A 作用后的信息熵的为 Info(D)，计算如下：

其中 k 表示样本 D 被分为 k 个部分。

决策树模型本身结构非常简单，效果也不错，但由于深度不易控制，容易出现过拟合和欠拟合的情况，需要配合相应的预剪枝和后剪枝技术，还需要相应的领域内经验对模型进行调整。
但是以决策树为基础的随机森林，boosting，GBDT等算法以其可解释性强，并行化能力高，分类效果好的特点，在学术界和工业界都大放异彩。



GBDT(Gradient Boost Decision Tree)
GBDT 梯度提升树算法近些年来应用广泛，
在工业界取得了非常显著的成就。 它是一种基于决策树的迭代集成算法，该算法由多颗决策树组成，最终由所有树的结论累加起来形成最终的分类结果。与单纯的决策树模型不同，GBDT算法不仅可以用来分类，也可以用于回归问题。

GBDT算法的构成部分不是普通的决策树，而是与分类树不同的用于回归问题的回归树，因为GBDT的核心就是累加所有决策树的结果作为最终结果，因此GBDT中所有的树都是回归树。
回归树的构建流程与普通决策树类似，但是在每个节点（包括叶子结点）都会得到一个预测值，这个预测值为这个节点每个属性所有值的平均值。分枝时穷举每个特征的每个阈值以找到最好的分割点。但衡量标准一般不是最大熵或者基尼系数，而是最小化均方差。
在这里可以将每一个回归树都看作是一个函数f(x)。GBDT把boost的思想用于森林的扩展，迭代使用多棵树共同决策。每一步的预测值y(t)是在父节点的预测值y(t-1)的基础上前进的，每一轮的迭代预测值依赖于t-1轮的结果。
y(it) = 
f(x)是一棵回归树，第t轮迭代对样本xi的预测值等于第t-1轮迭代的预测值加上f(t)对xi的预测值，问题就是：

类比二阶泰勒展开式：
令gi= hi=
得到 loss 二次函数在ft(xi)=-gi/hi处取得极小值。
当loss损失函数取平方误差函数时，所以GBDT是一种基于残差的训练方法。
